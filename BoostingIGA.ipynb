{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from ipykernel import kernelapp as app\n",
    "from scipy import stats\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, optimizers\n",
    "from keras.models import Model, model_from_json, Sequential\n",
    "from keras.layers import Dense, Input, BatchNormalization, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "import BoostedIGA as BIGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0  0.516399  0.570668  0.028474  0.171522  0.685277  0.833897  0.306966   \n",
      "1  0.554228  0.352132  0.181892  0.785602  0.965483  0.232354  0.083561   \n",
      "2  0.685306  0.517867  0.048485  0.137869  0.186967  0.994318  0.520665   \n",
      "3  0.913154  0.807920  0.402998  0.357224  0.952877  0.343632  0.865100   \n",
      "4  0.097146  0.102847  0.701507  0.890480  0.159560  0.275573  0.672492   \n",
      "\n",
      "         x8        x9       x10  \n",
      "0  0.893613  0.721544  0.189939  \n",
      "1  0.603548  0.728993  0.276239  \n",
      "2  0.578790  0.734819  0.541962  \n",
      "3  0.830278  0.538161  0.922469  \n",
      "4  0.164303  0.701371  0.487635  \n",
      "           y\n",
      "0  17.579365\n",
      "1  20.461479\n",
      "2  15.369717\n",
      "3  15.862597\n",
      "4  10.828538\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "boston_df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston_df['MEDV'] = boston_dataset.target\n",
    "\n",
    "X = boston_df.drop(['MEDV'], axis = 1)\n",
    "y = boston_df.drop(X.columns, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(features):\n",
    "    # id and label (not features)\n",
    "    unused_feature_list = ['parcelid']\n",
    "\n",
    "    # too many missing\n",
    "    missing_list = ['framing_id', 'architecture_style_id', 'story_id', 'perimeter_area', 'basement_sqft', 'storage_sqft'\n",
    "                   ]\n",
    "    unused_feature_list += missing_list\n",
    "\n",
    "    # not useful\n",
    "    bad_feature_list = ['fireplace_flag', 'deck_id', 'pool_unk_1', 'construction_id', 'fips', 'county_id',\n",
    "                        'Unnamed: 0', 'missing'\n",
    "                       ]\n",
    "    #['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag']\n",
    "    unused_feature_list += bad_feature_list\n",
    "\n",
    "    # hurts performance\n",
    "    unused_feature_list += ['county_landuse_code_id', 'zoning_description_id']\n",
    "\n",
    "    return features.drop(unused_feature_list, axis=1, errors='ignore')\n",
    "\n",
    "def impute_data(df):\n",
    "    values = {'quality_id':stats.mode(df.loc[(df.quality_id.isnull()==False),'quality_id'].values)[0][0],\n",
    "              'finished_area_sqft_calc': df.loc[(df.finished_area_sqft_calc.isnull() == False),'finished_area_sqft_calc'].values.mean(),\n",
    "              'lot_sqft': df.loc[(df.lot_sqft.isnull() == False),'lot_sqft'].values.mean(),\n",
    "              'census_1': df.loc[(df.census_1.isnull() == False),'census_1'].values.mean(), \n",
    "              'bathroom_small_cnt':0,\n",
    "              'unit_cnt': stats.mode(df.loc[(df.unit_cnt.isnull() == False),'unit_cnt'].values)[0][0],\n",
    "              'patio_sqft':df.loc[(df.patio_sqft.isnull() == False),'patio_sqft'].values.mean(),\n",
    "              'tax_property':df.loc[(df.tax_property.isnull() == False),'tax_property'].values.mean(),\n",
    "              'census_2': df.loc[(df.census_2.isnull() == False),'census_2'].values.mean(),\n",
    "             }\n",
    "    df = df.fillna(value=values)\n",
    "    \n",
    "    mask = (df.pool_cnt >= 1)\n",
    "    df.loc[mask,'pool_total_size'] = df.loc[(df.pool_total_size.isnull() == False),'pool_total_size'].values.mean()\n",
    "    \n",
    "    mask = (df.garage_cnt >= 1)\n",
    "    df.loc[mask,'garage_cnt'] = df.loc[(df.garage_sqft.isnull()==False),'garage_sqft'].values.mean()\n",
    "    \n",
    "    mask_0 = (df.tax_property >0)\n",
    "    mask_1 = (df.finished_area_sqft_calc > 0)\n",
    "    mask_null = (df.property_tax_per_sqft.isnull() == True)\n",
    "    df.loc[mask_null,'property_tax_per_sqft']= df.loc[mask_0,'tax_property'] / df.loc[mask_1,'finished_area_sqft_calc']\n",
    "\n",
    "    mask_0 = (df.avg_area_per_room >0)\n",
    "    mask_null = (df.avg_area_per_room.isnull() == True)\n",
    "    df.loc[mask_null,'avg_area_per_room'] = df.loc[mask_1,'finished_area_sqft_calc'] / df.loc[df.room_cnt>0, 'room_cnt']\n",
    "    \n",
    "    df.loc[np.isfinite(df.avg_garage_size) == False] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_test_features(features_2016, features_2017):\n",
    "    test_features_2016 = catboost_drop_features(features_2016)\n",
    "    test_features_2017 = catboost_drop_features(features_2017)\n",
    "    \n",
    "    test_features_2016['year'] = 0\n",
    "    test_features_2017['year'] = 1\n",
    "    \n",
    "    # 11 and 12 lead to bad results, probably due to the fact that there aren't many training examples for those two\n",
    "    test_features_2016['month'] = 10\n",
    "    test_features_2017['month'] = 10\n",
    "    \n",
    "    test_features_2016['quarter'] = 4\n",
    "    test_features_2017['quarter'] = 4\n",
    "    \n",
    "    return test_features_2016, test_features_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_df = pd.read_csv('zillow_2016.csv', index_col=0)\n",
    "\n",
    "drop_list = ['bathroom_cnt','bedroom_cnt','latitude','longitude','room_cnt','tax_year',\n",
    "            'year_built','location_1','location_2','location_3','location_4','derived_room_cnt',\n",
    "            'avg_area_per_room']\n",
    "\n",
    "#zillow_features = impute_data(zillow_df)\n",
    "\n",
    "for c in zillow_df.columns:\n",
    "    if zillow_df[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(zillow_df[c].fillna(-1).values))\n",
    "        zillow_df[c] = lbl.transform(list(zillow_df[c].fillna(-1).values))\n",
    "\n",
    "zillow_features = drop_features(zillow_df)\n",
    "\n",
    "FS = ['bathroom_cnt', 'bedroom_cnt', 'quality_id', 'floor1_sqft',\n",
    "       'finished_area_sqft_calc', 'floor1_sqft_unk', 'base_total_area',\n",
    "       'fireplace_cnt', 'bathroom_full_cnt', 'garage_cnt', 'garage_sqft',\n",
    "       'heating_id', 'latitude', 'longitude', 'pool_total_size',\n",
    "       'landuse_type_id', 'census_1', 'city_id', 'neighborhood_id',\n",
    "       'patio_sqft', 'year_built', 'story_cnt', 'tax_structure', 'tax_parcel',\n",
    "       'tax_property', 'tax_overdue_year', 'census_2', 'location_1',\n",
    "       'location_2', 'location_3', 'location_4', 'missing_total_area',\n",
    "       'derived_room_cnt', 'avg_area_per_room', 'derived_avg_area_per_room',\n",
    "       'month', 'quarter']\n",
    "\n",
    "#zillow_features.dropna(axis=0, subset = drop_list, inplace=True)\n",
    "#zillow_features.fillna(-1.0, inplace= True)\n",
    "\n",
    "zillow_label = zillow_df.logerror.astype(np.float32)\n",
    "#prepare\n",
    "\n",
    "X = zillow_features.drop(['logerror'],axis=1)\n",
    "#X = zillow_features[FS]\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y = zillow_label.values.reshape(-1,1)\n",
    "#y = zillow_norm(y,zillow_label.min(),zillow_label.max(),0,1)\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer= Imputer()\n",
    "imputer.fit(X.iloc[:, :])\n",
    "X_train = imputer.transform(X.iloc[:, :])\n",
    "\n",
    "X = pd.DataFrame(X_train, columns = X.columns)\n",
    "\"\"\"\n",
    "#normData, (scaler_x, scaler_y) = normalize(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [s for s in X.columns]\n",
    "cat_features = ['cooling_id', 'heating_id', 'landuse_type_id', 'year', 'month', 'quarter']\n",
    "\n",
    "categorical_indices = []\n",
    "for i, n in enumerate(X.columns):\n",
    "    if n in cat_features:\n",
    "        categorical_indices.append(i)\n",
    "        X[n] = X[n].astype(np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zosmex/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the best model: \n",
      "{'batch_size': 32, 'epochs': 150}\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "scoring = {\n",
    "    'MAAPE':make_scorer(mean_arctan_absolute_percentage_error, greater_is_better=False),\n",
    "    'MSE':'neg_mean_squared_error'}\n",
    "\n",
    "\n",
    "model = keras.wrappers.scikit_learn.KerasRegressor(build_fn=create_model,\n",
    "                        epochs=10, \n",
    "                        batch_size=5,\n",
    "                        verbose=0)\n",
    "\n",
    "param_grid = {'epochs':[50,100,150],\n",
    "              'batch_size':[16,32,64]}\n",
    "\n",
    "grid = GridSearchCV(model,\n",
    "                    param_grid=param_grid,\n",
    "                    return_train_score=True,\n",
    "                    scoring=scoring,\n",
    "                    refit = 'MSE')\n",
    "\n",
    "grid_results = grid.fit(normData[0], y)\n",
    "\n",
    "print('Parameters of the best model: ')\n",
    "print(grid_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy for -0.009246 using {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.001}\n",
      "mean=0.03319, std=0.005137 using {'batch_size': 16, 'epochs': 50, 'learning_rate': 0.01}\n",
      "mean=0.05543, std=0.004372 using {'batch_size': 16, 'epochs': 50, 'learning_rate': 0.005}\n",
      "mean=0.08032, std=0.005013 using {'batch_size': 16, 'epochs': 50, 'learning_rate': 0.001}\n",
      "mean=0.1012, std=0.004867 using {'batch_size': 16, 'epochs': 100, 'learning_rate': 0.01}\n",
      "mean=0.1269, std=0.003253 using {'batch_size': 16, 'epochs': 100, 'learning_rate': 0.005}\n",
      "mean=0.1497, std=0.00592 using {'batch_size': 16, 'epochs': 100, 'learning_rate': 0.001}\n",
      "mean=0.1737, std=0.002343 using {'batch_size': 16, 'epochs': 150, 'learning_rate': 0.01}\n",
      "mean=0.2006, std=0.005892 using {'batch_size': 16, 'epochs': 150, 'learning_rate': 0.005}\n",
      "mean=0.2205, std=0.004274 using {'batch_size': 16, 'epochs': 150, 'learning_rate': 0.001}\n",
      "mean=0.237, std=0.005691 using {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.01}\n",
      "mean=0.2663, std=0.005759 using {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.005}\n",
      "mean=0.2878, std=0.005458 using {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.001}\n",
      "mean=0.3355, std=0.02023 using {'batch_size': 32, 'epochs': 100, 'learning_rate': 0.01}\n",
      "mean=0.34, std=0.02224 using {'batch_size': 32, 'epochs': 100, 'learning_rate': 0.005}\n",
      "mean=0.3892, std=0.03278 using {'batch_size': 32, 'epochs': 100, 'learning_rate': 0.001}\n",
      "mean=0.4095, std=0.03045 using {'batch_size': 32, 'epochs': 150, 'learning_rate': 0.01}\n",
      "mean=0.4331, std=0.006391 using {'batch_size': 32, 'epochs': 150, 'learning_rate': 0.005}\n",
      "mean=0.4485, std=0.02452 using {'batch_size': 32, 'epochs': 150, 'learning_rate': 0.001}\n",
      "mean=0.4746, std=0.008651 using {'batch_size': 64, 'epochs': 50, 'learning_rate': 0.01}\n",
      "mean=0.4859, std=0.02135 using {'batch_size': 64, 'epochs': 50, 'learning_rate': 0.005}\n",
      "mean=0.5122, std=0.01051 using {'batch_size': 64, 'epochs': 50, 'learning_rate': 0.001}\n",
      "mean=0.5585, std=0.005042 using {'batch_size': 64, 'epochs': 100, 'learning_rate': 0.01}\n",
      "mean=0.5958, std=0.02379 using {'batch_size': 64, 'epochs': 100, 'learning_rate': 0.005}\n",
      "mean=0.5936, std=0.01782 using {'batch_size': 64, 'epochs': 100, 'learning_rate': 0.001}\n",
      "mean=0.6441, std=0.01501 using {'batch_size': 64, 'epochs': 150, 'learning_rate': 0.01}\n",
      "mean=0.6399, std=0.005598 using {'batch_size': 64, 'epochs': 150, 'learning_rate': 0.005}\n",
      "mean=0.6699, std=0.008737 using {'batch_size': 64, 'epochs': 150, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Accuracy for {grid_results.best_score_:.4} using {grid_results.best_params_}')\n",
    "means = grid_results.cv_results_['mean_score_time']\n",
    "stds = grid_results.cv_results_['std_score_time']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Best_score = {'RMSE':0, \n",
    "              'MAAPE':0, \n",
    "              'Iteration':0,\n",
    "              'Features':[]\n",
    "             }\n",
    "Score_t = []\n",
    "D_t = []\n",
    "MAPE_score = []\n",
    "MAAPE_score = []\n",
    "MAE_score = []\n",
    "RMSE_score = []\n",
    "\n",
    "#init weight\n",
    "D_t.append([1/X.shape[0]] * X.shape[0])\n",
    "\n",
    "for t in range(0,X.shape[1]):\n",
    "    print('Feature Selection x Boosting >>> Iteration: %d' % (t+1))\n",
    "    if (t == 0):\n",
    "        Xs = pd.DataFrame(data = normData[0], columns = X.columns)\n",
    "        \n",
    "    nw1 = create_model(len(Xs))\n",
    "    #feature selection\n",
    "    nw1.fit(\n",
    "            Xs, y, \n",
    "            epochs = grid_results.best_params_['epochs'], \n",
    "            batch_size=grid_results.best_params_['batch_size'], \n",
    "            verbose = 0\n",
    "           )\n",
    "    W = nw1.layers[1].get_weights()[0]\n",
    "    V = nw1.layers[2].get_weights()[0]\n",
    "    print(\"Feature Selection Process: %d\" % (t+1))\n",
    "    ranked_features = IGA(W, V, Xs.values, D_t[t], Xs.columns) \n",
    "    ranked_features.reset_index(inplace = True)\n",
    "    Xs.drop([ranked_features['Feature'][0]], axis = 1, inplace = True)\n",
    "    X_sel = X.drop(Xs.columns, axis = 1)\n",
    "    print(\"Selected Feature: \",(X_sel.columns))\n",
    "    \n",
    "\n",
    "    #Boosting\n",
    "    nw2, ss_maape, ss_rmse  = train_Shuffle(X_sel,y, \n",
    "                                            batch_size=grid_results.best_params_['batch_size'], \n",
    "                                            epochs=grid_results.best_params_['epochs'],\n",
    "                                            n_split = 5\n",
    "                                           )\n",
    "    \n",
    "    print(\"Avg MAAPE: %.5f%% (+/- %.5f%%)\" % (np.mean(ss_maape), np.std(ss_maape)))\n",
    "    print('Avg RMSE: %.5f%% (+/- %.5f%%)' % (np.mean(ss_rmse), np.std(ss_rmse)))\n",
    "    \n",
    "    MAAPE_score.append(np.mean(ss_maape))\n",
    "    RMSE_score.append(np.mean(ss_rmse))\n",
    "\n",
    "    D_t.append(update_weight_R2(nw2.predict(X_sel), y, D_t[t]))\n",
    "    \n",
    "    if(np.mean(ss_maape) < Best_score[0][0]):\n",
    "        print('New record!')\n",
    "\n",
    "        Best_score['MAAPE'] = np.mean(ss_maape)\n",
    "        Best_score['RMSE'] = np.mean(ss_rmse)\n",
    "        Best_score['Iteration'] = t\n",
    "        Best_score['Features'] = X_sel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(1,X.shape[1]+1), MAE_score)\n",
    "print(Best_score[3].values)\n",
    "print(Best_score[2])\n",
    "print(Best_score[0])\n",
    "print(Best_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-selection model baseline\n",
    "\n",
    "model, ss_maape, ss_rmse = train_Shuffle(\n",
    "                                        X, y, \n",
    "                                        batch_size=grid_results.best_params_['batch_size'], \n",
    "                                        epoch = grid_results.best_params_['epochs'],\n",
    "                                        n_split = 5,\n",
    "                                        norm = True\n",
    "                                        )\n",
    "\n",
    "print(\"Avg MAAPE: %.5f%% (+/- %.3f%%)\" % (np.mean(ss_maape), np.std(ss_maape)))\n",
    "print(\"Avg RMSE: %.5f%% (+/- %.3f%%)\" % (np.mean(ss_rmse), np.std(ss_rmse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test set\n",
    "prop_2016 = pd.read_csv('prop_2016.csv')\n",
    "\n",
    "sample = pd.read_csv('sample_sub.csv')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "sample = sample.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "df_test = sample.merge(prop_2016, on='parcelid', how='left')\n",
    "\n",
    "#add datetime\n",
    "df_test['year'] = zillow_df['year']\n",
    "df_test['month'] = zillow_df['month']\n",
    "df_test['quarter'] = zillow_df['quarter']\n",
    "#df.drop()\n",
    "for c in df_test.columns:\n",
    "    if df_test[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df_test[c].fillna(-1).values))\n",
    "        df_test[c] = lbl.transform(list(df_test[c].fillna(-1).values))\n",
    "        \n",
    "#df_test = impute_data(df_test)\n",
    "df_test = drop_features(df_test)\n",
    "df_test = df_test[X.columns]\n",
    "#df_test.fillna(-1.0, inplace= True)\n",
    "#df_test = df_test[zillow_features.drop('logerror',axis=1).columns.values]\n",
    "for i, n in enumerate(df_test.columns):\n",
    "    if n in cat_features:\n",
    "        df_test[n] = df_test[n].astype(np.str)\n",
    "        \n",
    "FS = ['bathroom_cnt', 'bedroom_cnt', 'quality_id', 'floor1_sqft',\n",
    "       'finished_area_sqft_calc', 'floor1_sqft_unk', 'base_total_area',\n",
    "       'fireplace_cnt', 'bathroom_full_cnt', 'garage_cnt', 'garage_sqft',\n",
    "       'heating_id', 'latitude', 'longitude', 'pool_total_size',\n",
    "       'landuse_type_id', 'census_1', 'city_id', 'neighborhood_id',\n",
    "       'patio_sqft', 'year_built', 'story_cnt', 'tax_structure', 'tax_parcel',\n",
    "       'tax_property', 'tax_overdue_year', 'census_2', 'location_1',\n",
    "       'location_2', 'location_3', 'location_4', 'missing_total_area',\n",
    "       'derived_room_cnt', 'avg_area_per_room', 'derived_avg_area_per_room',\n",
    "       'month', 'quarter']\n",
    "\n",
    "#df_test = df_test[FS]\n",
    "\n",
    "x_test = df_test.values\n",
    "#df_test = df_test[X.columns]\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer= Imputer()\n",
    "imputer.fit(df_test.iloc[:, :])\n",
    "x_test = imputer.transform(df_test.iloc[:, :])\n",
    "\"\"\"\n",
    "#print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.flatten()\n",
    "#y_pred = scaler_y.inverse_transform(y_pred_ann).flatten()\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': prop_2016['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "\n",
    "output.to_csv('model/score/submission_02_CatBoost.csv', index=False)\n",
    "print(output.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test set\n",
    "prop_2016 = pd.read_csv('prop_2016.csv')\n",
    "\n",
    "sample = pd.read_csv('sample_sub.csv')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "sample = sample.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "df_test = sample.merge(prop_2016, on='parcelid', how='left')\n",
    "\n",
    "#add datetime\n",
    "df_test['year'] = zillow_df['year']\n",
    "df_test['month'] = zillow_df['month']\n",
    "df_test['quarter'] = zillow_df['quarter']\n",
    "\n",
    "for c in df_test.columns:\n",
    "    if df_test[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df_test[c].fillna(-1).values))\n",
    "        df_test[c] = lbl.transform(list(df_test[c].fillna(-1).values))\n",
    "        \n",
    "#df_test = impute_data(df_test)\n",
    "df_test = drop_features(df_test)\n",
    "#df_test = df_test[zillow_features.drop('logerror',axis=1).columns.values]\n",
    "\n",
    "\"\"\"\n",
    "df_test = df_test[['bathroom_cnt', 'bedroom_cnt', 'floor1_sqft', 'finished_area_sqft_calc',\n",
    "       'floor1_sqft_unk', 'bathroom_full_cnt', 'latitude', 'longitude',\n",
    "       'region_zip', 'unit_cnt', 'tax_structure', 'tax_parcel', 'tax_land',\n",
    "       'tax_property', 'property_tax_per_sqft', 'location_1', 'location_2',\n",
    "       'location_3', 'location_4', 'derived_room_cnt', 'avg_area_per_room']]\n",
    "\"\"\"\n",
    "df_test = df_test[['bathroom_cnt', 'bedroom_cnt', 'quality_id', 'floor1_sqft',\n",
    "       'finished_area_sqft_calc', 'floor1_sqft_unk', 'base_total_area',\n",
    "       'fireplace_cnt', 'bathroom_full_cnt', 'garage_cnt', 'garage_sqft',\n",
    "       'heating_id', 'latitude', 'longitude', 'pool_total_size',\n",
    "       'landuse_type_id', 'census_1', 'city_id', 'neighborhood_id',\n",
    "       'patio_sqft', 'year_built', 'story_cnt', 'tax_structure', 'tax_parcel',\n",
    "       'tax_property', 'tax_overdue_year', 'census_2', 'location_1',\n",
    "       'location_2', 'location_3', 'location_4', 'missing_total_area',\n",
    "       'derived_room_cnt', 'avg_area_per_room', 'derived_avg_area_per_room',\n",
    "       'month', 'quarter']]\n",
    "\n",
    "#df_test = df_test[X.columns]\n",
    "#df_test.fillna(-1.0, inplace= True)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer= Imputer()\n",
    "imputer.fit(df_test)\n",
    "x_test = imputer.transform(df_test)\n",
    "\n",
    "#x_test = df_test[Best_score[3].values]\n",
    "\n",
    "#print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def create_model_zillow(normX=10, optimizer='adam', learning_rate = 0.001):\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units = 160 , kernel_initializer = 'he_normal', activation = 'tanh', input_dim = normX.shape[1]))\n",
    "    nn.add(Dense(units = 80 , kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "    nn.add(Dense(units = 25 , kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "    nn.add(Dense(1, kernel_initializer='he_normal'))\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    nn.compile(loss='mae', optimizer=adam)\n",
    "\n",
    "    #print(model.summary())\n",
    "    return nn\n",
    "\n",
    "FS = df_test.columns\n",
    "\n",
    "Xs = X[FS]\n",
    "\n",
    "sc = StandardScaler()\n",
    "Xs = sc.fit_transform(X[FS])\n",
    "\n",
    "model = create_model_zillow(Xs)\n",
    "\"\"\"\n",
    "cv = KFold(n_splits=4, random_state=42, shuffle=False)\n",
    "for train_index, test_index in cv.split(Xs):\n",
    "    X_train, X_test, y_train, y_test = Xs[train_index], Xs[test_index], y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train ,epochs=60, verbose=2, batch_size=32)\n",
    "    print('------End of fold---------')\n",
    "\"\"\"\n",
    "\n",
    "#normData, Scaler = normalize(Xs, y)\n",
    "\n",
    "model.fit(Xs, y, epochs = 60, batch_size=32, verbose = 2)\n",
    "#y_pred = model.predict(x_test)\n",
    "\n",
    "#print(metrics.mean_squared_error(normY[test], pred,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = MinMaxScaler().fit_transform(x_test)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "#y_pred_ann = loaded_model.predict(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.flatten()\n",
    "#y_pred = scaler_y.inverse_transform(y_pred_ann).flatten()\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': prop_2016['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('model/score/submission_06_3-FS.csv', index=False)\n",
    "print(output.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAAPE_score = score[0]\n",
    "RMSE_score = score[1]\n",
    "MAE_score = score[2]\n",
    "\n",
    "print(X.shape)\n",
    "print(len(MAAPE_score))\n",
    "print(len(MAAPE_RFE))\n",
    "print(len(MAAPE_RLF))\n",
    "\n",
    "x_plt = range(1,X.shape[1]+1)\n",
    "y1 = MAAPE_score\n",
    "y2 = MAAPE_RFE\n",
    "y3 = MAAPE_RLF\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(3)\n",
    "ax[0].scatter(x_plt, y1, linewidth=2, c='green')\n",
    "ax[1].scatter(x_plt, y2, linewidth=2, marker = '^')\n",
    "ax[2].scatter(x_plt, y3, linewidth=2, c='red', marker = '+')\n",
    "\n",
    "for i, txt in enumerate(y1):\n",
    "    ax[0].annotate(s='%.3f'%y1[i], xy=(x_plt[i], y1[i]), xytext =(x_plt[i], y1[i]+0.5))\n",
    "    ax[1].annotate(s='%.3f'%y2[i], xy=(x_plt[i], y2[i]), xytext =(x_plt[i], y2[i]+0.5))\n",
    "    ax[2].annotate(s='%.3f'%y3[i], xy=(x_plt[i], y3[i]), xytext =(x_plt[i], y3[i]+0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Friedman\n",
    "score = [[30.473142470555143, 27.524494961588864, 26.605383547798986, 23.22201894753832, 15.27896139801174, 15.36745466501974, 15.711099321309906, 15.72440426777695, 15.476193287964936, 16.127032362238708], [5.049074973674501, 4.551485525981321, 4.320769171530718, 3.804615991930487, 2.546364366210144, 2.619511759671777, 2.6712968881620966, 2.627200736599554, 2.5760268708290726, 2.6155109889741155], [4.104870167962773, 3.676849911772784, 3.509723176576311, 3.0744177133171386, 1.9530088838472508, 2.0300163041462653, 2.055415123054879, 2.077687303564899, 2.007485151557529, 2.0278159884068554]]\n",
    "comp_score = ([[30.473142470555143, 27.524494961588864, 26.605383547798986, 23.22201894753832, 15.27896139801174, 15.36745466501974, 15.711099321309906, 15.72440426777695, 15.476193287964936, 16.127032362238708], [5.049074973674501, 4.551485525981321, 4.320769171530718, 3.804615991930487, 2.546364366210144, 2.619511759671777, 2.6712968881620966, 2.627200736599554, 2.5760268708290726, 2.6155109889741155], [4.104870167962773, 3.676849911772784, 3.509723176576311, 3.0744177133171386, 1.9530088838472508, 2.0300163041462653, 2.055415123054879, 2.077687303564899, 2.007485151557529, 2.0278159884068554]], [[[26.55372129318402, 21.881879098311042, 18.111132191390166, 15.358869570274464, 15.820994934186828, 16.309656175237414, 15.528439099502176, 15.827722810709627, 16.020385590257167, 15.858275496878587], [4.2103912774873065, 3.4719257065339284, 2.9478176558982163, 2.5718204605288655, 2.604625462867344, 2.7068780391950265, 2.588527680957001, 2.606214539563855, 2.6884476055062225, 2.6140861015572776], [3.4677701423071654, 2.805577983714497, 2.341127548497288, 2.0042473374892174, 2.0367794520028735, 2.103645841729993, 2.005077985338816, 2.0268774602921793, 2.0710612564211077, 1.9893827088903238]]], [[[29.933526726938254, 30.136473180118106, 30.33066703827501, 30.075184152073348, 29.196192868059093, 27.929515699260776, 26.740085995113578, 20.45699571587513, 21.223858383283602, 15.675444064475759], [4.2103912774873065, 3.4719257065339284, 2.9478176558982163, 2.5718204605288655, 2.604625462867344, 2.7068780391950265, 2.588527680957001, 2.606214539563855, 2.6884476055062225, 2.6140861015572776], [3.4677701423071654, 2.805577983714497, 2.341127548497288, 2.0042473374892174, 2.0367794520028735, 2.103645841729993, 2.005077985338816, 2.0268774602921793, 2.0710612564211077, 1.9893827088903238]]])\n",
    "\n",
    "MAAPE_score = score[0]\n",
    "RMSE_score = score[1]\n",
    "MAE_score = score[2]\n",
    "\n",
    "RFE_comp = comp_score[1]\n",
    "RMSE_RFE = comp_score[1][0][1]\n",
    "\n",
    "x1 = ['x1','x2','x4','x4','x5']\n",
    "y1 = RMSE_score[:5]\n",
    "y2 = RMSE_RFE[:5]\n",
    "y3 = RMSE_RLF[:5]\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(3)\n",
    "ax[0].scatter(x_plt, y1, linewidth=2, c='green')\n",
    "ax[0].set_xlim([0.5,5.5])\n",
    "ax[1].scatter(x_plt, y2, linewidth=2, marker='^')\n",
    "ax[1].set_xlim([0.5,5.5])\n",
    "ax[2].scatter(x_plt, y3, linewidth=2, marker='+', c='red')\n",
    "ax[2].set_xlim([0.5,5.5])\n",
    "\n",
    "\n",
    "labels_1 = ['','x3','x2','x5','x1','x4']\n",
    "ax[0].set_xticklabels(labels_1)\n",
    "\n",
    "labels_1 = ['','x4','x2','x1','x5','x10']\n",
    "ax[1].set_xticklabels(labels_1)\n",
    "\n",
    "for i, txt in enumerate(y1):\n",
    "    ax[0].annotate(s='%.3f'%y1[i], xy=(x_plt[i], y1[i]), xytext =(x_plt[i], y1[i]+0.2))\n",
    "    ax[1].annotate(s='%.3f'%y2[i], xy=(x_plt[i], y2[i]), xytext =(x_plt[i], y2[i]+0.2))\n",
    "    ax[2].annotate(s='%.3f'%y3[i], xy=(x_plt[i], y3[i]), xytext =(x_plt[i], y3[i]+0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection : RFE\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "# load data\n",
    "#dataframe = house_df.drop(['SalePrice'],axis = 1)\n",
    "#dataframe.drop(['ProvinceID','DistrictID','UserType','PropertyType'], axis = 1, inplace = True)\n",
    "#target = house_df['SalePrice']\n",
    "\n",
    "RFE_FS = []\n",
    "high_score_RFE=0\n",
    "nof_RFE = 0\n",
    "MAAPE_RFE = []\n",
    "RMSE_RFE = []\n",
    "MAE_RFE = []\n",
    "RFE_Score = []\n",
    "sel_f = np.nan\n",
    "# feature extraction\n",
    "for k in range(0, X.shape[1]):\n",
    "    print(\"Ite :\", k+1)\n",
    "    model = linear_model.LinearRegression()\n",
    "    rfe = RFE(model,k+1)\n",
    "    X_rfe = rfe.fit_transform(normData[0], normData[1])\n",
    "    temp = pd.Series(rfe.support_, index = X.columns)\n",
    "    selected_features_rfe = temp[temp==True].index\n",
    "    print(selected_features_rfe)\n",
    "\n",
    "    model, history_RFE, res, RFE_mape, RFE_maape, RFE_rmse, RFE_mae = train_Shuffle(X_rfe, y, 150, 16,norm = True)\n",
    "\n",
    "    print(\"Avg MAAPE: %.2f%% (+/- %.2f%%)\" % (np.mean(RFE_maape), np.std(RFE_maape)))\n",
    "    print(\"Avg RMSE: %.2f%% (+/- %.2f%%)\" % (np.mean(RFE_rmse), np.std(RFE_rmse)))\n",
    "    print(\"Avg MAE: %.2f%% (+/- %.2f%%)\" % (np.mean(RFE_mae), np.std(RFE_mae)))\n",
    "    \n",
    "    MAAPE_RFE.append(np.mean(RFE_maape))\n",
    "    RMSE_RFE.append(np.mean(RFE_rmse))\n",
    "    MAE_RFE.append(np.mean(RFE_mae))\n",
    "    RFE_FS.append(temp)\n",
    "    if(np.mean(RFE_rmse) < high_score_RFE):\n",
    "        print('New Record!')\n",
    "        high_score_RFE = np.mean(RFE_rmse)\n",
    "        nof_RFE = X_rfe.shape[1]\n",
    "        sel_f = selected_features_rfe\n",
    "        \n",
    "RFE_Score = [MAAPE_RFE, RMSE_RFE, MAE_RFE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(high_score_RFE)\n",
    "print(nof_RFE)\n",
    "print(sel_f)\n",
    "\n",
    "plt.scatter(range(1,len(r2_RFE)+1), rmse_RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn_relief as relief\n",
    "\n",
    "y_list = []\n",
    "for i in y.values:\n",
    "    y_list.append(i[0])\n",
    "y_list = np.asarray(y_list)\n",
    "\n",
    "high_score_RLF = 100\n",
    "nof_RLF = 0\n",
    "sel_RLF = []\n",
    "MAAPE_RLF = []\n",
    "RMSE_RLF = []\n",
    "MAPE_RLF = []\n",
    "MAE_RLF = []\n",
    "RLF_Score = []\n",
    "\n",
    "for k in range(0,X.shape[1]):\n",
    "    print('Iteration: ', k+1)\n",
    "    r = relief.RReliefF(n_jobs=1, n_features = k+1)\n",
    "    X_RLF = r.fit_transform(X.values, y_list)\n",
    "    sel_RLF.append(X_RLF)\n",
    "\n",
    "    model_RLF, history_RLF, res, RLF_mape, RLF_maape, RLF_rmse, RLF_mae = train_Shuffle(X_RLF, y, 100,16)\n",
    "    \n",
    "    print(\"Avg MAAPE: %.2f%% (+/- %.2f%%)\" % (np.mean(RLF_maape), np.std(RLF_maape)))\n",
    "    print(\"Avg RMSE: %.2f%% (+/- %.2f%%)\" % (np.mean(RLF_rmse), np.std(RLF_rmse)))\n",
    "    \n",
    "    RMSE_RLF.append(np.mean(RLF_rmse))\n",
    "    MAPE_RLF.append(np.mean(RLF_mape))\n",
    "    MAAPE_RLF.append(np.mean(RLF_maape))\n",
    "    MAE_RLF.append(np.mean(RLF_mae))\n",
    "    if(np.mean(RLF_rmse) < high_score_RLF):\n",
    "        print('New Record!')\n",
    "        high_score_RLF = np.mean(RLF_rmse)\n",
    "        nof_RLF = k\n",
    "\n",
    "RLF_Score = [MAAPE_RLF, RMSE_RLF, MAE_RLF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores = []\n",
    "RLF_Score = [MAAPE_RLF, RMSE_RLF, MAE_RLF]\n",
    "RFE_Score = [[[25.272627511637463,21.174547376864002,17.7255826783602,15.651749607165133,16.455264568189722,15.630521473014335,17.075491796367615,15.559287132699529,17.311902544858057,15.374725296968132],[4.1359020402279345,3.4572107607236555,2.855058535957814,2.4631819795571217,2.6538084828702626,2.5270956763719603,2.7123168325586544,2.5950615464680595,2.7969309531917426,2.553069299888645],[3.422283506529446,2.739075642052992,2.2423720075785685,1.9547090107450167,2.059485032023333,1.9648064177913915,2.149548692290976,2.009688620488025,2.19583138148212,1.9822889501347647]]]\n",
    "IGarson_Score = [[30.473142470555143, 27.524494961588864, 26.605383547798986, 23.22201894753832, 15.27896139801174, 15.36745466501974, 15.711099321309906, 15.72440426777695, 15.476193287964936, 16.127032362238708], [5.049074973674501, 4.551485525981321, 4.320769171530718, 3.804615991930487, 2.546364366210144, 2.619511759671777, 2.6712968881620966, 2.627200736599554, 2.5760268708290726, 2.6155109889741155], [4.104870167962773, 3.676849911772784, 3.509723176576311, 3.0744177133171386, 1.9530088838472508, 2.0300163041462653, 2.055415123054879, 2.077687303564899, 2.007485151557529, 2.0278159884068554]]\n",
    "\n",
    "Scores = IGarson_Score, RFE_Score, RLF_Score\n",
    "\n",
    "file = open(\"model/Comparing_Score_Friedman.txt\",\"w\")\n",
    "file.write(str(Scores))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLF_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unselected Model\")\n",
    "print('Accuracy = %.5f with %d features' % (unsel_score, X.shape[1]))\n",
    "print(\"Avg MAPE: %.5f%% (+/- %.3f%%)\" % (np.mean(ss_mape), np.std(ss_mape)))\n",
    "\n",
    "print(\"\\nBoosting x IGarson\")\n",
    "print('Accuracy = %.5f with %d features' % (Best_score[0][0], Best_score[2]))\n",
    "\n",
    "print(\"\\nRLF\")\n",
    "print(\"Accuracy = %.5f with %d features\" %(high_score_RLF, nof_RLF))\n",
    "\n",
    "print(\"\\nRFE\")\n",
    "print(\"Accuracy = %.5f with %d features\" %(high_score_RFE, nof_RFE)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
